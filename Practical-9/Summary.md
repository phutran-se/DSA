The runtime results demonstrate the performance characteristics of various sorting algorithms, aligning with their theoretical time complexities. BubbleSort, InsertionSort, and SelectionSort, all O(N²), exhibit significant performance degradation as array size increases. BubbleSort is the slowest on large descending arrays (68,736.02 ms for N=20,000), due to excessive swaps. InsertionSort performs best among O(N²) algorithms on nearly sorted data (8.08 ms for N=1,000), leveraging its O(N) best-case efficiency, but struggles with descending arrays (49,949.12 ms for N=20,000). SelectionSort maintains consistent performance (around 17,000–18,000 ms for N=20,000) but does not benefit from partially sorted data, making it less practical. MergeSort consistently achieves O(N log N) complexity, with stable runtimes across all data types (125.52–146.77 ms for N=20,000), though its non-in-place nature requires additional memory. QuickSort with left-most pivot suffers worst-case O(N²) behavior on ascending and descending arrays (13,699.94 ms and 27,419.10 ms for N=20,000), due to unbalanced partitions, posing a stack overflow risk for larger arrays. QuickSortMedian3 and QuickSortRandom mitigate this, achieving near O(N log N) performance (58.75 ms and 76.84 ms for N=20,000 random), with QuickSortMedian3 being the fastest due to its robust pivot selection. On random and nearly sorted data, QuickSort variants outperform MergeSort slightly, benefiting from in-place sorting. These results confirm the theoretical advantages of O(N log N) algorithms, with QuickSortMedian3 being the most efficient for general-purpose sorting, while MergeSort is preferable for stability and predictability